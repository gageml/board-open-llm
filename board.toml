title = "Open LLM Leaderboard"
description = "Imported results from [Open LLM Leaderboard](http://tinyurl.com/2l5uspcp)."

[run-select]

operation = "eval-model"
status = "completed"

[group-select]

group-by = { attribute = "model" }
max = { run-attr = "started" }

[[columns]]

run-attr = "id"
label = "Run ID"
description = """
Unique identifier of the Gage run that imported the model eval results.
"""
hide = true

[[columns.links]]

# TODO - link to actual help page on runs
href = "https://github.com/gageml/gage"
type = "help"
label = "Help with Gage Runs"

[[columns]]

run-attr = "operation"
hide = true

[[columns]]

run-attr = "status"
label = "Run Status"
hide = true

[[columns]]

attribute = "model"

[[columns.links]]

href = "https://huggingface.co/$1"
label = "Hugging Face Model Card"
type = "model"

[[columns]]

attribute = "precision"
hide = true

[[columns]]

attribute = "params"
label = "Params (B)"
filter = "range"
hide = true

[[columns]]

metric = "arc"
label = "ARC"
description = """
ARC can be seen as a general artificial intelligence benchmark, as a
program synthesis benchmark, or as a psychometric intelligence test.
It is targeted at both humans and artificially intelligent systems that
aim at emulating a human-like form of general fluid intelligence.
"""
sort = "desc"

    [[columns.links]]

    href = "https://arxiv.org/abs/1911.01547"
    label = "On the Measure of Intelligence"
    type = "paper"

    [[columns.links]]

    href = "https://github.com/fchollet/ARC"
    label = "Task Data"
    type = "dataset"

    [[columns.links]]

    href = "https://paperswithcode.com/sota/common-sense-reasoning-on-arc-challenge"
    label = "State-of-the-Art"
    type = "leaderboard"

[[columns]]

metric = "hellaswag"
label = "HellaSwag"
description = """
HellaSwag is a challenge dataset for evaluating commonsense NLI that
is specially hard for state-of-the-art models, though its questions
are trivial for humans (>95% accuracy).
"""

    [[columns.links]]

    href = "https://arxiv.org/abs/1905.07830v1"
    label = "HellaSwag: Can a Machine Really Finish Your Sentence?"
    type = "paper"

    [[columns.links]]

    href = "https://paperswithcode.com/dataset/hellaswag"
    label = "Dataset"
    type = "dataset"

    [[columns.links]]

    href = "https://paperswithcode.com/sota/sentence-completion-on-hellaswag"
    label = "State-of-the-Art"
    type = "leaderboard"

[[columns]]

metric = "mmlu"
label = "MMLU"
description = """
57 tasks including elementary mathematics, US history, computer science, law,
and more. To attain high accuracy on this test, models must possess extensive
world knowledge and problem solving ability.
"""

    [[columns.links]]

    href = "https://arxiv.org/abs/2009.03300"
    label = "Measuring Massive Multitask Language Understanding"
    type = "paper"

    [[columns.links]]

    href = "https://paperswithcode.com/dataset/mmlu"
    label = "Dataset"
    type = "dataset"

    [[columns.links]]

    href = "https://github.com/hendrycks/test"
    label = "Source code"
    type = "code"

    [[columns.links]]

    href = "https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu"
    label = "State-of-the-Art"
    type = "leaderboard"

[[columns]]

metric = "truthfulqa"
label = "TruthfulQA"
description = """
TruthfulQA is a benchmark to measure whether a language model is truthful in generating
answers to questions. The benchmark comprises 817 questions that span 38 categories,
including health, law, finance and politics. The authors crafted questions that some
humans would answer falsely due to a false belief or misconception.
"""

    [[columns.links]]

    href = "https://arxiv.org/abs/2109.07958"
    label = "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
    type = "paper"

    [[columns.links]]

    href = "https://paperswithcode.com/dataset/truthfulqa"
    label = "Dataset"
    type = "dataset"

    [[columns.links]]

    href = "https://github.com/sylinrl/TruthfulQA"
    label = "Source code"
    type = "code"

    [[columns.links]]

    href = "https://paperswithcode.com/sota/question-answering-on-truthfulqa"
    label = "State-of-the-Art"
    type = "leaderboard"

[[columns]]

metric = "winogrande"
label = "Winogrande"
description = """
WinoGrande is a large-scale dataset of 44k problems, inspired by the original
WSC design, but adjusted to improve both the scale and the hardness of the dataset.
The key steps of the dataset construction consist of (1) a carefully designed
crowdsourcing procedure, followed by (2) systematic bias reduction using a novel
AfLite algorithm that generalizes human-detectable word associations to
machine-detectable embedding associations.
"""

    [[columns.links]]

    href = "https://arxiv.org/abs/1907.10641"
    label = "WinoGrande: An Adversarial Winograd Schema Challenge at Scale"
    type = "paper"

    [[columns.links]]

    href = "https://paperswithcode.com/dataset/winogrande"
    label = "Dataset"
    type = "dataset"

    [[columns.links]]

    href = "https://github.com/allenai/winogrande"
    label = "Source code"
    type = "code"

    [[columns.links]]

    href = "https://paperswithcode.com/sota/common-sense-reasoning-on-winogrande"
    label = "State-of-the-Art"
    type = "leaderboard"

[[columns]]

metric = "gsm8k"
label = "GSM8K"
description = """
GSM8K consists of 8.5K high quality grade school math problems created by human
problem writers. These problems take between 2 and 8 steps to solve, and solutions
primarily involve performing a sequence of elementary calculations using basic
arithmetic operations to reach the final answer. A bright middle school student
should be able to solve every problem.
"""

    [[columns.links]]

    href = "https://arxiv.org/abs/2110.14168"
    label = "Training Verifiers to Solve Math Word Problems"
    type = "paper"

    [[columns.links]]

    href = "https://paperswithcode.com/dataset/gsm8k"
    label = "Dataset"
    type = "dataset"

    [[columns.links]]

    href = "https://github.com/openai/grade-school-math"
    label = "Source code"
    type = "code"

    [[columns.links]]

    href = "https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k"
    label = "State-of-the-Art"
    type = "leaderboard"
